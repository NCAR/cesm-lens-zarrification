{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: https://github.com/NCAR/cesm-lens-aws/issues/34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import intake\n",
    "from tqdm.auto import tqdm\n",
    "import shutil \n",
    "import os\n",
    "from functools import reduce\n",
    "import pprint\n",
    "import json\n",
    "from operator import mul\n",
    "import random\n",
    "import yaml\n",
    "from distributed.utils import format_bytes\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import cftime\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calendar Conversion functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for converting single date objects from one type to another.\n",
    "\n",
    "def convert_to_noleap(cftime360_obj, datemap):\n",
    "    ''' Convert Date from 360 Day to NoLeap'''\n",
    "    newdate = datemap[cftime360_obj.dayofyr - 1]\n",
    "    converted = cftime.DatetimeNoLeap(year=cftime360_obj.year, month=newdate.month, day=newdate.day)\n",
    "    return converted\n",
    "\n",
    "def convert_to_gregorian(cftime_noleap_obj):\n",
    "    ''' Convert Date from NoLeap to Gregorian '''\n",
    "    converted = cftime.DatetimeGregorian(year=cftime_noleap_obj.year, month=cftime_noleap_obj.month, day=cftime_noleap_obj.day)\n",
    "    return converted\n",
    "\n",
    "def convert_hour(time_obj, hour_of_day):\n",
    "    ''' Convert date object to Gregorian and explicitly set the hour of day.'''\n",
    "    time_obj = cftime.DatetimeGregorian(year=time_obj.year, month=time_obj.month, day=time_obj.day, hour=hour_of_day, minute=0, second=0)\n",
    "    return time_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datemap_360_to_noleap():\n",
    "    ''' Return an array of dates mapping days from the 360-Day calendar to the No-Leap calendar. '''\n",
    "\n",
    "    # Choose any year with 365 days. \n",
    "    dummy_year = 1999\n",
    "\n",
    "    # These are the days of the year that will be missing on the time axis for each year.\n",
    "    # The goal is to spread missing dates out evenly over each year.\n",
    "    #\n",
    "    # Modify specific dates as desired. \n",
    "    missing_dates = [date(dummy_year, 1, 31),\n",
    "                     date(dummy_year, 3, 31),\n",
    "                     date(dummy_year, 5, 31),\n",
    "                     date(dummy_year, 8, 31),\n",
    "                     date(dummy_year, 10, 31),]\n",
    "    \n",
    "    day_one = date(dummy_year, 1, 1)\n",
    "    missing_dates_indexes = [(day - day_one).days + 1 for day in missing_dates] \n",
    "    missing_dates_indexes\n",
    "\n",
    "    datemap_indexes = np.setdiff1d(np.arange(365), missing_dates_indexes)\n",
    "    datemap_indexes\n",
    "\n",
    "    dates = pd.date_range(f'1/1/{dummy_year}', f'12/31/{dummy_year}')\n",
    "    assert(len(dates) == 365)\n",
    "    \n",
    "    date_map = dates[datemap_indexes]\n",
    "    assert(len(date_map) == 360)\n",
    "    \n",
    "    # Check to make sure February 29 is not a date in the resulting map.\n",
    "    #is_leap_day = [(d.month == 2) and (d.day == 29) for d in date_map]\n",
    "    #print(is_leap_day)\n",
    "    #assert(not any(is_leap_day))\n",
    "    return date_map\n",
    "\n",
    "\n",
    "# Create a global map for moving days of the year to other days of the year.\n",
    "datemap_global = get_datemap_360_to_noleap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code \"pads out\" data variables with missing values for Leap Days.  \n",
    "# It's possible that xarray will do this automatically as long as one calendar being merged has Leap Days in it.\n",
    "\n",
    "def convert_dataset_noleap_to_gregorian(ds):\n",
    "    '''Converts an xarray dataset from the NoLeap calendar to the Gregorian calendar.  \n",
    "       Data for Leap Days are filled with missing values (np.nan).\n",
    "    '''\n",
    "    # Convert dates in the original dataset from the NoLeap to Gregorian calendar\n",
    "    ds['time'] = [convert_to_gregorian(t) for t in ds.time.values]\n",
    "    \n",
    "    # Create an equivalent date range on the Gregorian calendar\n",
    "    start_date = ds.time.values[0]\n",
    "    end_date = ds.time.values[-1]\n",
    "    times = xr.DataArray(xr.cftime_range(start=start_date, end=end_date, freq='D', calendar='gregorian', normalize=True), dims='time')\n",
    "    \n",
    "    # Find the leap days in this date range.\n",
    "    is_leap_day = (times.time.dt.month == 2) & (times.time.dt.day == 29)\n",
    "    leap_days = times.where(is_leap_day, drop=True)\n",
    "    \n",
    "    # Create fill values for these days.\n",
    "    one_time_step = ds.isel(time=slice(0, 1))\n",
    "    fill_values = []\n",
    "    for leap_day in leap_days:\n",
    "        d = xr.full_like(one_time_step,fill_value=np.nan)\n",
    "        d = d.assign_coords(time=[leap_day.data])\n",
    "        fill_values.append(d)\n",
    "    \n",
    "    # Append the fill values to the dataset and then sort values by time.\n",
    "    fill_values.append(ds)\n",
    "\n",
    "    ds_fixed=xr.concat(fill_values, dim='time').sortby('time')\n",
    "    return ds_fixed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run These Cells for Dask Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from distributed import Client\n",
    "dask.config.set({'distributed.dashboard.link': '/proxy/{port}/status'})\n",
    "dask.config.get('distributed.dashboard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min_jobs = 20\n",
    "#cluster = SLURMCluster(cores=4, memory=\"50GB\", project=\"STDD0003\")\n",
    "min_jobs = 4\n",
    "cluster = SLURMCluster(cores=4, memory=\"50GB\")\n",
    "cluster.adapt(minimum_jobs=min_jobs, maximum_jobs=35)\n",
    "#cluster.scale(jobs=3)\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True if saving large Zarr files is resulting in KilledWorker or Dask crashes.\n",
    "BIG_SAVE = True\n",
    "if BIG_SAVE:\n",
    "    min_workers = min_jobs\n",
    "    print('Waiting for ' + str(min_jobs) + ' workers.')\n",
    "    client.wait_for_workers(min_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Notebook Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare individual dataset for merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(ds):\n",
    "    \"\"\"This function gets called on each original dataset before concatenation.\n",
    "       Convert all dataset calendars to Gregorian.  \n",
    "       For now, also drop other data variables, like time bounds, until we get things looking good.\n",
    "    \"\"\"\n",
    "\n",
    "    # Print dataset title for debug purposes\n",
    "    #print(ds.attrs['title'])\n",
    "    #print(f'ds.time.attrs = {ds.time.attrs}')\n",
    "    #print(f'ds.time.encoding = {ds.time.encoding}')\n",
    "\n",
    "    attrs = ds.time.attrs\n",
    "    encoding = ds.time.encoding\n",
    "    bounds_name = ds.time.attrs['bounds']\n",
    "    \n",
    "    ds_fixed = ds\n",
    "    #\"\"\"Drop all unneeded variables and coordinates\"\"\"\n",
    "    #vars_to_drop = [vname for vname in ds.data_vars if vname not in variables]\n",
    "    #coord_vars = [vname for vname in ds.data_vars if 'time' not in ds[vname].dims or 'bnd' in vname]\n",
    "    #ds_fixed = ds.set_coords(coord_vars)\n",
    "    #data_vars_dims = []\n",
    "    #for data_var in ds_fixed.data_vars:\n",
    "    #    data_vars_dims.extend(list(ds_fixed[data_var].dims))\n",
    "    #coords_to_drop = [coord for coord in ds_fixed.coords if coord not in data_vars_dims]\n",
    "    #grid_vars = list(set(vars_to_drop + coords_to_drop) - set(['time', 'time_bound']))\n",
    "    #ds_fixed = ds_fixed.drop(grid_vars)\n",
    "    #if 'history' in ds_fixed.attrs:\n",
    "    #    del ds_fixed.attrs['history']\n",
    "    \n",
    "    # Print some diagnostic information on the dataset.\n",
    "    #print_ds_info(ds, 'tasmax')\n",
    "    \n",
    "    # Test for calendar type xarray found when it loaded the dataset.\n",
    "    time_type = f'{type(ds.time.values[0])}'\n",
    "    has_360_day_calendar = \"Datetime360Day\" in time_type\n",
    "    has_noleap_calendar = \"DatetimeNoLeap\" in time_type\n",
    "    \n",
    "    # Extract the time_bnds variable for conversion\n",
    "    bnds = ds_fixed[bounds_name].values\n",
    "\n",
    "    if has_360_day_calendar:\n",
    "        print(f'Found 360 day calendar; converting dates to NoLeap, then date types to Gregorian.\\n')\n",
    "        ds_fixed['time'] = [convert_to_noleap(t, datemap_global) for t in ds_fixed.time.values]\n",
    "        ds_fixed['time'] = [convert_to_gregorian(t) for t in ds_fixed.time.values]\n",
    "\n",
    "        bnds = [[convert_to_noleap(col, datemap_global) for col in row] for row in bnds]\n",
    "        bnds = [[convert_to_gregorian(col) for col in row] for row in bnds]\n",
    "        #ds_fixed = convert_dataset_noleap_to_gregorian(ds_fixed)\n",
    "\n",
    "    # Convert any NoLeap calendar to the Gregorian calendar.\n",
    "    elif has_noleap_calendar:\n",
    "        ds_fixed['time'] = [convert_to_gregorian(t) for t in ds_fixed.time.values]\n",
    "        bnds = [[convert_to_gregorian(col) for col in row] for row in bnds]\n",
    "        #ds_fixed = convert_dataset_noleap_to_gregorian(ds_fixed)\n",
    "\n",
    "    # Change time of day to noon for all time axis points.\n",
    "    #print(ds_fixed.time.values.shape)\n",
    "    ds_fixed['time'] = [convert_hour(t, 12) for t in ds_fixed.time.values]\n",
    "    bnds = [[convert_hour(col, 0) for col in row] for row in bnds]\n",
    "    ds_fixed[bounds_name] = (('time', 'bnds'), bnds)\n",
    "    \n",
    "    # Convert CFTimeIndex to Pandas DateTimeIndex\n",
    "    if type(ds_fixed.time.indexes['time'] == 'Index'):\n",
    "        print('found Index object; converting to CFTimeIndex object.\\n')\n",
    "        datetimeindex = xr.CFTimeIndex(ds_fixed.time.indexes['time']).to_datetimeindex()\n",
    "        ds.assign_coords(time = datetimeindex)\n",
    "        \n",
    "    ds.time.attrs = attrs\n",
    "    ds.time.encoding = encoding\n",
    "    ds = ds.set_coords([bounds_name])\n",
    "\n",
    "    return ds_fixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merged dataset processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_time(\n",
    "    ds,\n",
    "    start,\n",
    "    end,\n",
    "    freq,\n",
    "    time_bounds_dim,\n",
    "    calendar='standard',\n",
    "    generate_bounds=True,\n",
    "    instantaneous=False,\n",
    "):\n",
    "    '''Regenerate time axis to be consistent with time bounds variable'''\n",
    "    \n",
    "    ds = ds.sortby('time').copy()\n",
    "    attrs = ds.time.attrs\n",
    "    encoding = ds.time.encoding\n",
    "    bounds_name = ds.time.attrs['bounds']\n",
    "    ds[bounds_name].load()\n",
    "    if generate_bounds:\n",
    "        times = xr.cftime_range(\n",
    "            start=start, end=end, freq=freq, calendar=calendar\n",
    "        )\n",
    "        bounds = np.vstack([times[:-1], times[1:]]).T\n",
    "        ds[bounds_name].data = bounds\n",
    "\n",
    "    if instantaneous:\n",
    "        ds = ds.assign_coords(time=ds[bounds_name].min(time_bounds_dim))\n",
    "    else:\n",
    "        ds = ds.assign_coords(time=ds[bounds_name].mean(time_bounds_dim))\n",
    "    ds.time.attrs = attrs\n",
    "    ds.time.encoding = encoding\n",
    "    ds = ds.set_coords([bounds_name])\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_chunking(datasets, chunks, data_var):\n",
    "    \"\"\"Enforce uniform chunking in the Zarr Store.\n",
    "    \"\"\"\n",
    "    dsets = datasets.copy()\n",
    "    choice = random.choice(range(0, len(dsets)))\n",
    "    for i, (key, ds) in enumerate(dsets.items()):\n",
    "        print(f'key == {key}')\n",
    "        c = chunks.copy()\n",
    "        for dim in list(c):\n",
    "            if dim not in ds.dims:\n",
    "                del c[dim]\n",
    "        ds = ds.chunk(c)\n",
    "        keys_to_delete = ['intake_esm_dataset_key', 'intake_esm_varname']\n",
    "        for k in keys_to_delete:\n",
    "            del ds.attrs[k]\n",
    "        dsets[key] = ds\n",
    "        #variable = key.split(field_separator)[-1]\n",
    "        #print_ds_info(ds, variable)\n",
    "        print_ds_info(ds, data_var)\n",
    "        if i == choice:\n",
    "            print(ds)\n",
    "        print('\\n')\n",
    "    return dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ds_info(ds, var):\n",
    "    \"\"\"Function for printing chunking information\"\"\"\n",
    "\n",
    "    print(f'print_ds_info: var == {var}')\n",
    "    dt = ds[var].dtype\n",
    "    itemsize = dt.itemsize\n",
    "    chunk_size = ds[var].data.chunksize\n",
    "    size = format_bytes(ds.nbytes)\n",
    "    _bytes = reduce(mul, chunk_size) * itemsize\n",
    "    chunk_size_bytes = format_bytes(_bytes)\n",
    "\n",
    "    print(f'Variable name: {var}')\n",
    "    print(f'Dataset dimensions: {ds[var].dims}')\n",
    "    print(f'Chunk shape: {chunk_size}')\n",
    "    print(f'Dataset shape: {ds[var].shape}')\n",
    "    print(f'Chunk size: {chunk_size_bytes}')\n",
    "    print(f'Dataset size: {size}')\n",
    "\n",
    "# For now, make the Zarr output directory a global variable.\n",
    "dirout = './zarr-stores'\n",
    "\n",
    "def zarr_store(var, exp, frequency, grid, biascorrection, write=False, dirout=dirout):\n",
    "    \"\"\" Create zarr store name/path\n",
    "    \"\"\"\n",
    "    path = f'{dirout}/{var}.{exp}.{frequency}.{grid}.{biascorrection}.zarr'\n",
    "    if write and os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "    print(path)\n",
    "    return path\n",
    "\n",
    "\n",
    "def save_data(ds, store):\n",
    "    try:\n",
    "        ds.to_zarr(store, consolidated=True)\n",
    "        del ds\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to write {store}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metadata preparation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_metadata(ds, member_id):\n",
    "    '''Convert dataset metadata to dictionary form.\n",
    "    '''\n",
    "    m_dict = {}\n",
    "    for key, value in ds.attrs.items():\n",
    "        m_dict[key] = {member_id: value}\n",
    "    return m_dict\n",
    "\n",
    "def get_metadata_from_catalog_entries(catalog_entries):\n",
    "    '''Take a catalog subset and combine all global dataset metadata into one dictionary.\n",
    "    '''\n",
    "    metadata = {}\n",
    "\n",
    "    # Loop over catalog rows\n",
    "    dataframe = catalog_entries.df\n",
    "    for path, member_id in zip(dataframe['path'], dataframe['member_id']):\n",
    "\n",
    "        ds = xr.open_dataset(path)\n",
    "        ds_metadata = get_dataset_metadata(ds, member_id)\n",
    "\n",
    "        # Loop over metadata entries in dataset\n",
    "        for key, value in ds_metadata.items():\n",
    "            if key in metadata:\n",
    "                metadata[key].update(value)\n",
    "            else:\n",
    "                metadata[key] = value\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def save_metadata_to_csv(metadata_dict, variable_name):\n",
    "    '''Save metadata in dictionary form to a csv file. '''\n",
    "    dataframe = pd.DataFrame.from_dict(metadata_dict)\n",
    "    dataframe.to_csv(f'{variable_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = get_metadata(ds, 'test')\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_metadata_dict = get_metadata_from_catalog_entries(col)\n",
    "save_metadata_to_csv(global_metadata_dict, \"tasmax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Zarr Stores Using the Catalog and Main Notebook Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's safer to use a underscore separator, because NA-CORDEX grids have dashes.\n",
    "field_separator = '_'\n",
    "col = intake.open_esm_datastore(\"./toy-na-cordex.json\", sep=field_separator)\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of isolating one entry from the catalog.\n",
    "#ds = col['/Users/bonnland/GitRepos/cesm-lens-zarrification/notebooks/na-cordex/data-subsets/subset_tasmax.rcp85.CanESM2.CRCM5-UQAM.day.NAM-22i.raw.nc_tasmax_rcp85_CanESM2_CRCM5-UQAM_day_NAM-22i_raw_common_CanESM2.CRCM5-UQAM'].to_dask()\n",
    "#dict(ds.dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard-code the variable name in a global variable for now.\n",
    "variables = ['tasmax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate datasets according to the catalog JSON metadata.\n",
    "chunks = {'time': 200, 'lat': -1, 'lon': -1}\n",
    "dsets = col.to_dataset_dict(cdf_kwargs={'chunks': chunks, 'use_cftime': True}, preprocess=preprocess, progressbar=False)\n",
    "dset = dsets['rcp85_day_NAM-22i_raw']\n",
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following line will place all ensemble members in the same chunk.   \n",
    "# Comment out to have each ensemble member in its own chunk.\n",
    "chunks['member_id'] = 1\n",
    "\n",
    "# Take care of ragged edges in original datasets, to optimize chunking strategy.\n",
    "dsets = enforce_chunking(dsets, chunks, variables[0])\n",
    "dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create/Overwrite the Zarr Stores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, ds in tqdm(dsets.items(), desc='Saving zarr store'):\n",
    "    print('key: ' + key)\n",
    "    key = key.split(field_separator)\n",
    "    exp, frequency, grid, biascorrection = key[0], key[1], key[2], key[3]\n",
    "    \n",
    "    # Regenerate the time bounds variable to be consistent across all ensemble members.\n",
    "    #\n",
    "    # start:  Move the starting bound backward from noon to midnight of the first day.\n",
    "    # end:    Create an extra day for the ending time bound of the last day, and set hour to midnight.\n",
    "    start = convert_hour((dset.time.values[0]), 0)\n",
    "    end = convert_hour(pd.to_datetime(dset.time.values[-1].strftime()) + pd.DateOffset(1), 0)\n",
    "    time_bounds_dim='time'\n",
    "    ds_fixed = fix_time(dset, start=start, end=end, freq='D', time_bounds_dim=time_bounds_dim).chunk(chunks)\n",
    "    \n",
    "    var = variables[0]\n",
    "    store = zarr_store(var, exp, frequency, grid, biascorrection, write=True, dirout=dirout)\n",
    "    print(store)\n",
    "    save_data(ds_fixed, store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the zarr stores were properly written\n",
    "\n",
    "from pathlib import Path\n",
    "p = Path(dirout)\n",
    "stores = list(p.rglob(\"*.zarr\"))\n",
    "for store in stores:\n",
    "    try:\n",
    "        ds = xr.open_zarr(store.as_posix(), consolidated=True)\n",
    "        print('\\n')\n",
    "        print(store)\n",
    "        print(ds)\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        print(store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If Using Dask on HPC, release the workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to print out details about the conda environment.\n",
    "# %load_ext watermark\n",
    "# %watermark -d -iv -m -g -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative to Using the Catalog for Preprocessing:  Load Datasets Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_folder = './data-subsets'\n",
    "fileList = os.listdir(subset_folder)\n",
    "fileList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "for f in fileList:\n",
    "    # Create xarray dataset from file.\n",
    "    filePath = f'{subset_folder}/{f}'\n",
    "    ds = xr.open_dataset(filePath, use_cftime=True)\n",
    "    print(filePath)\n",
    "    print(ds)\n",
    "    break\n",
    "    #preprocess(ds)\n",
    "        \n",
    "    datasets.append(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test preprocessing for 360-day calendars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test conditions for 360 calendars\n",
    "filePath = './data-subsets/subset_tasmax.rcp85.HadGEM2-ES.RegCM4.day.NAM-22i.raw.nc'\n",
    "ds = xr.open_dataset(filePath, use_cftime=True)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_processed = preprocess(ds)\n",
    "ds_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing Code Using the Configuration File \"config.yaml\"\n",
    "###  This is Not Yet Tested and Working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_variables(col, variable, scenario, frequency, grid, biascorrection, verbose=True):\n",
    "    query = dict(variable=variable, scenario=scenario, frequency=frequency, grid=grid, biascorrection=biascorrection)\n",
    "    subset = col.search(**query)\n",
    "    if verbose:\n",
    "        print(subset.unique(columns=['variable', 'scenario', 'frequency,', 'grid', 'biascorrection']))\n",
    "    return subset, query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "        \n",
    "variables = config['variables']\n",
    "frequencies = config['frequencies']\n",
    "scenarios = config['scenarios']\n",
    "biascorrections = config['biascorrections']\n",
    "grid_categories = config['grid_categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = []\n",
    "\n",
    "for key, value in grid_categories.items():\n",
    "    grid = value['grid']\n",
    "    chunks = value['chunks']\n",
    "    for scenario in scenarios:\n",
    "        for frequency in frequencies:\n",
    "            for biascorrection in biascorrections:\n",
    "                for variable in variables:\n",
    "                    col_subset, query = process_variables(col, variable, scenario, frequency, grid, biascorrection)\n",
    "                    d = {'query': json.dumps(query), 'col': col_subset, 'chunks': chunks, 'frequency': frequency}\n",
    "                    run_config.append(d)\n",
    "                    \n",
    "run_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = []\n",
    "\n",
    "\n",
    "variable_categories = list(config['variable_category'].keys())\n",
    "grid = config['grid']\n",
    "biascorrection = config['biascorrection']\n",
    "frequency = config['frequency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v_cat in variable_categories:\n",
    "    scenarios = list(config['variable_category'][v_cat]['scenario'].keys())\n",
    "    for scenario in scenarios:\n",
    "        print(scenario)\n",
    "        chunks = config['variable_category'][v_cat]['scenario'][scenario]['chunks']\n",
    "        variable = config['variable_category'][v_cat]['variable']\n",
    "        variables.extend(variable)\n",
    "        col_subset, query = process_variables(col, variable, scenario, grid, biascorrection)\n",
    "        d = {'query': json.dumps(query), 'col': col_subset, 'chunks': chunks, 'frequency': frequency}\n",
    "        run_config.append(d)\n",
    "                \n",
    "#print(variables)\n",
    "#print(run_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in run_config:\n",
    "    print(\"*\"*120)\n",
    "    print(f\"query = {run['query']}\")\n",
    "    frequency = run['frequency']\n",
    "    chunks = run['chunks']\n",
    "    # Try preprocessing, including calendar conversion.\n",
    "    dsets = run['col'].to_dataset_dict(cdf_kwargs={'chunks': chunks, 'decode_times': False}, preprocess=preprocess, progressbar=False)\n",
    "    # Turn off preprocessing for now.\n",
    "    #dsets = run['col'].to_dataset_dict(cdf_kwargs={'chunks': chunks, 'decode_times': False}, preprocess=None, progressbar=False)\n",
    "\n",
    "    # The following line will place all ensemble members in the same chunk.\n",
    "    #chunks['member_id'] = 1\n",
    "    #dsets = enforce_chunking(dsets, chunks)\n",
    "    \n",
    "    for key, ds in tqdm(dsets.items(), desc='Saving zarr store'):\n",
    "        print('key: ' + key)\n",
    "        key = key.split(field_separator)\n",
    "        exp, cmp, var, frequency = key[1], key[0], key[-1], frequency\n",
    "        store = zarr_store(exp, cmp, frequency, var, write=True, dirout=dirout)\n",
    "        save_data(ds, store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SANDBOX: Code Testing Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('/Users/bonnland/GitRepos/cesm-lens-zarrification/notebooks/na-cordex/data-subsets/subset_tasmax.rcp85.MPI-ESM-LR.WRF.day.NAM-22i.raw.nc')\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.data_vars)\n",
    "print(ds.attrs['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dates in the original dataset from the NoLeap to Gregorian calendar\n",
    "ds['time'] = [convert_to_gregorian(t) for t in ds.time.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a date range on the Gregorian calendar\n",
    "start_date = ds.time.values[0]\n",
    "end_date = ds.time.values[-1]\n",
    "\n",
    "times = xr.DataArray(xr.cftime_range(start=start_date, end=end_date, freq='D', calendar='gregorian'), dims='time')\n",
    "times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the leap days in this date range.\n",
    "is_leap_day = (times.time.dt.month == 2) & (times.time.dt.day == 29)\n",
    "leap_days = times.where(is_leap_day, drop=True)\n",
    "leap_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fill values for these days.\n",
    "one_time_step = ds['tasmax'].isel(time=slice(0, 1))\n",
    "fill_values = []\n",
    "for leap_day in leap_days:\n",
    "    d = xr.full_like(one_time_step,fill_value=np.nan)\n",
    "    d = d.assign_coords(time=[leap_day.data])\n",
    "    fill_values.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the fill values to the dataset and then sort values by time.\n",
    "fill_values.append(ds['tasmax'])\n",
    "\n",
    "ds_fixed=xr.concat(fill_values, dim='time').sortby('time')\n",
    "ds_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[dsets[key].get_index('time') for key in dsets][2][0]\n",
    "[dsets[key].get_index('time') for key in dsets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dsets.values())[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(dsets.values())[0].time.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.concat(list(dsets.values())[:3], dim='member_id', combine_attrs='drop', data_vars=['tasmax'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.time.values[0].replace(hour=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ds.time.indexes[\"time\"].to_datetimeindex())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following query to gather all data for one variable.\n",
    "#subset = col.search(variable='tasmax', scenario=['hist','rcp85'], grid='NAM-22i', frequency='day')\n",
    "subset = col.search(variable='tasmax', scenario=['rcp85'], grid='NAM-22i', frequency='day')\n",
    "\n",
    "# Use this to load some 360-day data for conversion to the Gregorian calendar.\n",
    "#subset = col.search(variable='tasmax', scenario=['hist'], grid='NAM-22i', frequency='day', driver='HadGEM2-ES')\n",
    "\n",
    "\n",
    "subset.unique(columns=['rcm', 'driver', 'biascorrection', 'common'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in subset.keys():\n",
    "    print(type(subset[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for strange metadata\n",
    "for key in tqdm(subset.keys()):\n",
    "    try:\n",
    "        subset[key](cdf_kwargs={'chunks': {}, 'decode_times': False}).to_dask()\n",
    "    except Exception as e:\n",
    "        print(f'\\tFile:{subset[key].df.path.tolist()} --- Exception: {e}', end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0182fc39e85a45219399e957fe691adb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "13761fa2b8324fd2a9ece35a71f4f6ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Saving zarr store: 100%",
       "layout": "IPY_MODEL_226f6576d26a4a04b7db1f63a81d67b0",
       "max": 7,
       "style": "IPY_MODEL_bc55885d178345d485a2e54827a47926",
       "value": 7
      }
     },
     "20afd54515c14fd5891b3c5fa2c379e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Scale",
       "layout": "IPY_MODEL_30f5f4c7ea0d4f0aa375e06be9293746",
       "style": "IPY_MODEL_dbc81ae788164df28222a95d09f8e05c"
      }
     },
     "2121b17971ce4a46aed3a0c3b0ea6481": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntTextModel",
      "state": {
       "description": "Maximum",
       "layout": "IPY_MODEL_30f5f4c7ea0d4f0aa375e06be9293746",
       "step": 1,
       "style": "IPY_MODEL_9b50ae9a7ab3471d85831948b79cfebc"
      }
     },
     "226f6576d26a4a04b7db1f63a81d67b0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "26fe76239ce948519e75acbe45e3a7cb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "2a0faccbfb8d4e5b95eb6aebe93d929d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "30f5f4c7ea0d4f0aa375e06be9293746": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "150px"
      }
     },
     "34223d7b416145089c93c7d8a58c044a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "35eff6daed234d15aea4688997555389": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "3b81dd47848d4579b0bc817dc9539cf2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Adapt",
       "layout": "IPY_MODEL_30f5f4c7ea0d4f0aa375e06be9293746",
       "style": "IPY_MODEL_3e98b72279604abd9bef4fc8ca32738a"
      }
     },
     "3e98b72279604abd9bef4fc8ca32738a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ButtonStyleModel",
      "state": {}
     },
     "410e6595ef11423f81b49b9193faf9c1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_ce928ec4d83c4ff6a0ad9d50fe578c03",
       "style": "IPY_MODEL_34223d7b416145089c93c7d8a58c044a",
       "value": "<h2>SLURMCluster</h2>"
      }
     },
     "57c8221c458645c48099db5f2be789dc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "AccordionModel",
      "state": {
       "_titles": {
        "0": "Manual Scaling",
        "1": "Adaptive Scaling"
       },
       "children": [
        "IPY_MODEL_9ae48b7119c14b828195d19844943a12",
        "IPY_MODEL_dc327235bd944aeab1aafc20a23ff1a3"
       ],
       "layout": "IPY_MODEL_d156020fe11b4f70b58b42d074f06ca8",
       "selected_index": null
      }
     },
     "5ac713a15f234a6ea183925a72ae9db5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "min_width": "150px"
      }
     },
     "5e3003728e0542babfcf6305e9cd0483": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_f47e3d9d819d4701b673db7e2d0e7145",
       "style": "IPY_MODEL_35eff6daed234d15aea4688997555389",
       "value": "<p><b>Dashboard: </b><a href=\"/proxy/8787/status\" target=\"_blank\">/proxy/8787/status</a></p>\n"
      }
     },
     "65d81f01eb2f47cc8bb719561dfad40b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_c2f9b26c82294089a73eff87c6acb165",
        "IPY_MODEL_7a8bbcc522c145f0bdb6cd336c3b12d8"
       ],
       "layout": "IPY_MODEL_8117606279b3474e8cf1503333509604"
      }
     },
     "6f9bca40e5174d90a00ed76cbb261c82": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9b2a064f16204221aeb43dfad9eedc79",
       "style": "IPY_MODEL_f1d996c5452d4183b9fa04c7c306a6c7",
       "value": " 7/7 [1:56:02&lt;00:00, 994.67s/it]"
      }
     },
     "761a1c358a2b48498a5e6f7744667bac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntTextModel",
      "state": {
       "description": "Workers",
       "layout": "IPY_MODEL_30f5f4c7ea0d4f0aa375e06be9293746",
       "step": 1,
       "style": "IPY_MODEL_0182fc39e85a45219399e957fe691adb"
      }
     },
     "7871cd76b7b246afb971da3961f6a154": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_d2013d29175443b28ac300ea006850e7",
        "IPY_MODEL_57c8221c458645c48099db5f2be789dc"
       ],
       "layout": "IPY_MODEL_ba0c5c3b77464f28ac8ab33f96986134"
      }
     },
     "7a8bbcc522c145f0bdb6cd336c3b12d8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_2a0faccbfb8d4e5b95eb6aebe93d929d",
       "style": "IPY_MODEL_ed072022e70a46998a953b46fa72cb82",
       "value": " 4/4 [11:33&lt;00:00, 173.32s/it]"
      }
     },
     "7faf3793e39c4094b2d03170c314c0a1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_13761fa2b8324fd2a9ece35a71f4f6ae",
        "IPY_MODEL_6f9bca40e5174d90a00ed76cbb261c82"
       ],
       "layout": "IPY_MODEL_aaeca4b0644a4cd59e0b03765c2c959e"
      }
     },
     "8117606279b3474e8cf1503333509604": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8bee2cc24f5844329fa26e404f7c2396": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "8eb6968752454c2a9e1b0e3de50be20f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9a3f9df3bd6b4d29ad030d6fe88c8eae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "9ae48b7119c14b828195d19844943a12": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_761a1c358a2b48498a5e6f7744667bac",
        "IPY_MODEL_20afd54515c14fd5891b3c5fa2c379e6"
       ],
       "layout": "IPY_MODEL_8bee2cc24f5844329fa26e404f7c2396"
      }
     },
     "9b2a064f16204221aeb43dfad9eedc79": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9b50ae9a7ab3471d85831948b79cfebc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "a1de09204dce4cf78671a06f49037f92": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "aaeca4b0644a4cd59e0b03765c2c959e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ba0c5c3b77464f28ac8ab33f96986134": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bc55885d178345d485a2e54827a47926": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "c2f9b26c82294089a73eff87c6acb165": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "description": "Saving zarr store: 100%",
       "layout": "IPY_MODEL_a1de09204dce4cf78671a06f49037f92",
       "max": 4,
       "style": "IPY_MODEL_f465e02eaf2b401c899fb9bd12265491",
       "value": 4
      }
     },
     "ce928ec4d83c4ff6a0ad9d50fe578c03": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d156020fe11b4f70b58b42d074f06ca8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "min_width": "500px"
      }
     },
     "d2013d29175443b28ac300ea006850e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_5ac713a15f234a6ea183925a72ae9db5",
       "style": "IPY_MODEL_26fe76239ce948519e75acbe45e3a7cb",
       "value": "\n<div>\n  <style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n  </style>\n  <table style=\"text-align: right;\">\n    <tr> <th>Workers</th> <td>0</td></tr>\n    <tr> <th>Cores</th> <td>0</td></tr>\n    <tr> <th>Memory</th> <td>0 B</td></tr>\n  </table>\n</div>\n"
      }
     },
     "dbc81ae788164df28222a95d09f8e05c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ButtonStyleModel",
      "state": {}
     },
     "dc327235bd944aeab1aafc20a23ff1a3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_ddf498d4803e4f72949a1eeb286b01e0",
        "IPY_MODEL_2121b17971ce4a46aed3a0c3b0ea6481",
        "IPY_MODEL_3b81dd47848d4579b0bc817dc9539cf2"
       ],
       "layout": "IPY_MODEL_8eb6968752454c2a9e1b0e3de50be20f"
      }
     },
     "ddf498d4803e4f72949a1eeb286b01e0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "IntTextModel",
      "state": {
       "description": "Minimum",
       "layout": "IPY_MODEL_30f5f4c7ea0d4f0aa375e06be9293746",
       "step": 1,
       "style": "IPY_MODEL_9a3f9df3bd6b4d29ad030d6fe88c8eae"
      }
     },
     "e205f5d2a7d6419ca82d3d5cbbbe3892": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_410e6595ef11423f81b49b9193faf9c1",
        "IPY_MODEL_7871cd76b7b246afb971da3961f6a154",
        "IPY_MODEL_5e3003728e0542babfcf6305e9cd0483"
       ],
       "layout": "IPY_MODEL_eb05b25522d245569255c51b368528af"
      }
     },
     "eb05b25522d245569255c51b368528af": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ed072022e70a46998a953b46fa72cb82": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "f1d996c5452d4183b9fa04c7c306a6c7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "f465e02eaf2b401c899fb9bd12265491": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": "initial"
      }
     },
     "f47e3d9d819d4701b673db7e2d0e7145": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
