{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import intake\n",
    "from tqdm.auto import tqdm\n",
    "import shutil \n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "from functools import reduce\n",
    "import pprint\n",
    "import json\n",
    "\n",
    "from distributed.utils import format_bytes\n",
    "\n",
    "#import numpy as np\n",
    "#import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run These Cells for Dask Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from ncar_jobqueue import NCARCluster\n",
    "\n",
    "# Processes is processes PER CORE.\n",
    "# This one works fine.\n",
    "#cluster = NCARCluster(cores=15, processes=1, memory='100GB', project='STDD0003')\n",
    "# This one also works, but occasionally hangs near the end.\n",
    "#cluster = NCARCluster(cores=10, processes=1, memory='50GB', project='STDD0003')\n",
    "\n",
    "num_jobs = 10\n",
    "walltime = \"1:00:00\"\n",
    "cluster = NCARCluster(cores=num_jobs, processes=1, memory='10GB', project='STDD0003', walltime=walltime)\n",
    "cluster.scale(jobs=num_jobs)\n",
    "\n",
    "from distributed import Client\n",
    "from distributed.utils import format_bytes\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for Creating Combined Zarr Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_metadata(ds_hist, ds_fut, scenario):\n",
    "    '''Take two Xarray datasets, combine their metadata, and add Zarr-specific metadata.'''\n",
    "    keys = set(ds_hist.attrs.keys())\n",
    "    keys = keys.union(set(ds_fut.attrs.keys()))\n",
    "\n",
    "    metadata = {}\n",
    "    for key in keys:\n",
    "        if (key in ds_hist.attrs) and (key in ds_fut.attrs):\n",
    "\n",
    "            # If both stores have identical metadata, assign the metadata unchanged.\n",
    "            if ds_hist.attrs[key] == ds_fut.attrs[key]:\n",
    "                metadata[key] = ds_hist.attrs[key]\n",
    "            else:\n",
    "                # Otherwise, place both versions in a new dictionary.\n",
    "                metadata[key] = {'hist': ds_hist.attrs[key], scenario: ds_fut.attrs[key]}\n",
    "\n",
    "        elif key in ds_hist.attrs:\n",
    "            metadata[key] = {'hist': ds_hist.attrs[key]}\n",
    "\n",
    "        else:\n",
    "            metadata[key] = {scenario: ds_fut.attrs[key]}\n",
    "\n",
    "    metadata['zarr-dataset-reference'] = 'For dataset documentation, see DOI https://doi.org/10.5065/D6SJ1JCH'\n",
    "    metadata['zarr-note-time'] = f'Historical data runs 1950 to 2005, future data ({scenario}) runs 2006 to 2100.'\n",
    "    metadata['zarr-version'] = '1.0'\n",
    "    return metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Dataset Diagnostic Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ds_info(ds, var):\n",
    "    \"\"\"Function for printing chunking information\"\"\"\n",
    "\n",
    "    print(f'print_ds_info: var == {var}')\n",
    "    dt = ds[var].dtype\n",
    "    itemsize = dt.itemsize\n",
    "    chunk_size = ds[var].data.chunksize\n",
    "    size = format_bytes(ds.nbytes)\n",
    "    _bytes = reduce(mul, chunk_size) * itemsize\n",
    "    chunk_size_bytes = format_bytes(_bytes)\n",
    "\n",
    "    print(f'Variable name: {var}')\n",
    "    print(f'Dataset dimensions: {ds[var].dims}')\n",
    "    print(f'Chunk shape: {chunk_size}')\n",
    "    print(f'Dataset shape: {ds[var].shape}')\n",
    "    print(f'Chunk size: {chunk_size_bytes}')\n",
    "    print(f'Dataset size: {size}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Zarr Save Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(ds, store):\n",
    "    try:\n",
    "        ds.to_zarr(store, consolidated=True)\n",
    "        del ds\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to write {store}: {e}\")\n",
    "\n",
    "        \n",
    "def zarr_check():\n",
    "    '''Make sure the zarr stores were properly written'''\n",
    "\n",
    "    p = Path(dirout)\n",
    "    stores = list(p.rglob(\"*.zarr\"))\n",
    "    for store in stores:\n",
    "        try:\n",
    "            ds = xr.open_zarr(store.as_posix(), consolidated=True)\n",
    "            print('\\n')\n",
    "            print(store)\n",
    "            print(ds)\n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            print(store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find and Process Zarr Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_directory = '/glade/scratch/bonnland/na-cordex/zarr/'\n",
    "output_directory = '/glade/scratch/bonnland/na-cordex/zarr-publish/'\n",
    "\n",
    "#scenario = 'rcp45'\n",
    "scenario = 'rcp85'\n",
    "\n",
    "p = Path(input_directory)\n",
    "input_stores = list(p.glob(f'*.{scenario}.*.zarr'))\n",
    "\n",
    "WRITE_OUTPUT = False\n",
    "\n",
    "for store in input_stores:\n",
    "    future_store = store.as_posix()\n",
    "    historical_store = future_store.replace(scenario, 'hist')\n",
    "\n",
    "    # Determine the output store name and location.\n",
    "    output_store_name = future_store.replace(scenario, 'hist-' + scenario)\n",
    "    output_store_name = output_store_name.split('/')[-1]\n",
    "    output_store = output_directory + output_store_name\n",
    "\n",
    "    if WRITE_OUTPUT:\n",
    "        # Produce output store if it does not exist yet\n",
    "        if not os.path.exists(output_store):\n",
    "            os.makedirs(output_store)\n",
    "        else:\n",
    "            # Store exists; skip to the next case.\n",
    "            continue\n",
    "\n",
    "    ds_hist = xr.open_zarr(historical_store, consolidated=True)\n",
    "    ds_fut = xr.open_zarr(future_store, consolidated=True)\n",
    "    \n",
    "    hist_vars = list(ds_hist.data_vars.keys())\n",
    "    fut_vars = list(ds_fut.data_vars.keys())\n",
    " \n",
    "    # Verify the data variables are the same for both datasets, and there is only one variable.\n",
    "    assert(hist_vars == fut_vars)\n",
    "    assert(len(hist_vars) == 1)\n",
    "    data_var = hist_vars[0]\n",
    "    \n",
    "    # Print some diagnostic info to get that warm, fuzzy feeling.\n",
    "    #print_ds_info(ds_hist, hist_vars[0])\n",
    "    #print_ds_info(ds_fut, fut_vars[0])\n",
    "\n",
    "    # Verify that the data variable chunk sizes match for both datasets\n",
    "    assert(ds_hist[data_var].data.chunksize == ds_fut[data_var].data.chunksize)\n",
    "    \n",
    "    print(ds_hist[data_var].data.chunks)\n",
    "    \n",
    "    metadata = combine_metadata(ds_hist, ds_fut, scenario)\n",
    "    #print(f'\\n\\nMetadata for {output_store}:\\n')\n",
    "    #pprint.pprint(metadata, width=150, compact=True)\n",
    "\n",
    "    if WRITE_OUTPUT:\n",
    "        # Combine stores\n",
    "        ds_out = xr.concat([ds_hist, ds_fut], dim='time').sortby('time')\n",
    "\n",
    "        # Assign final metadata\n",
    "        ds_out.attrs = metadata\n",
    "\n",
    "        # De-fragment chunks along the time dimension.\n",
    "        ds_out.chunk(ds_hist[data_var].data.chunks)\n",
    "\n",
    "        # Write the store.\n",
    "        print(f'\\n\\n  Writing store: {output_store}...')\n",
    "        save_data(ds_out, output_store)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
