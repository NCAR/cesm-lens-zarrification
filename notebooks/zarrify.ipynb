{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: https://github.com/NCAR/cesm-lens-aws/issues/34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import intake\n",
    "from tqdm.auto import tqdm\n",
    "import shutil \n",
    "import os\n",
    "from functools import reduce\n",
    "import pprint\n",
    "import json\n",
    "from operator import mul\n",
    "import random\n",
    "import yaml\n",
    "from distributed.utils import format_bytes\n",
    "import dask\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from distributed import Client\n",
    "dask.config.set({'distributed.dashboard.link': '/proxy/{port}/status'})\n",
    "dask.config.get('distributed.dashboard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = SLURMCluster(cores=4, memory=\"120GB\")\n",
    "cluster.adapt(maximum_jobs=200)\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ds_info(ds, var):\n",
    "    \"\"\"Function for printing chunking information\"\"\"\n",
    "    dt = ds[var].dtype\n",
    "    itemsize = dt.itemsize\n",
    "    chunk_size = ds[var].data.chunksize\n",
    "    size = format_bytes(ds.nbytes)\n",
    "    _bytes = reduce(mul, chunk_size) * itemsize\n",
    "    chunk_size_bytes = format_bytes(_bytes)\n",
    "\n",
    "    print(f'Variable name: {var}')\n",
    "    print(f'Dataset dimensions: {ds[var].dims}')\n",
    "    print(f'Chunk shape: {chunk_size}')\n",
    "    print(f'Dataset shape: {ds[var].shape}')\n",
    "    print(f'Chunk size: {chunk_size_bytes}')\n",
    "    print(f'Dataset size: {size}')\n",
    "    \n",
    "\n",
    "dirout = \"/glade/scratch/abanihi/lens-aws\"\n",
    "def zarr_store(exp, cmp, frequency, var, write=False, dirout=dirout):\n",
    "    \"\"\" Create zarr store name/path\n",
    "    \"\"\"\n",
    "    path = f'{dirout}/{cmp}/{frequency}/cesmLE-{exp}-{var}.zarr'\n",
    "    if write and os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "    print(path)\n",
    "    return path\n",
    "\n",
    "\n",
    "def save_data(ds, store):\n",
    "    try:\n",
    "        ds.to_zarr(store, consolidated=True)\n",
    "        del ds\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to write {store}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = intake.open_esm_datastore(\"../catalogs/glade-campaign-cesm1-le.json\", sep='_',)\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_variables(col, variables, experiment, verbose=False):\n",
    "    query = dict(component=component, stream=stream, variable=variables, experiment=experiment)\n",
    "    subset = col.search(**query)\n",
    "    if verbose:\n",
    "        print(subset.unique(columns=['variable', 'component', 'stream', 'experiment']))\n",
    "    return subset, query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "run_config = []\n",
    "variables = []\n",
    "\n",
    "for component, stream_val in config.items():\n",
    "    for stream, v in stream_val.items():\n",
    "        frequency = v['frequency']\n",
    "        variable_categories = list(v['variable_category'].keys())\n",
    "        for v_cat in variable_categories:    \n",
    "            experiments = list(v['variable_category'][v_cat]['experiment'].keys())\n",
    "            for exp in experiments:\n",
    "                chunks = v['variable_category'][v_cat]['experiment'][exp]['chunks']\n",
    "                variable = v['variable_category'][v_cat]['variable']\n",
    "                variables.extend(variable)\n",
    "                col_subset, query = process_variables(col, variable , exp)\n",
    "                d = {'query': json.dumps(query), 'col': col_subset, 'chunks': chunks, 'frequency': frequency}\n",
    "                run_config.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_chunking(datasets, chunks):\n",
    "    \"\"\"Enforce uniform chunking\"\"\"\n",
    "    dsets = datasets.copy()\n",
    "    choice = random.choice(range(0, len(dsets)))\n",
    "    for i, (key, ds) in enumerate(dsets.items()):\n",
    "        c = chunks.copy()\n",
    "        for dim in list(c):\n",
    "            if dim not in ds.dims:\n",
    "                del c[dim]\n",
    "        ds = ds.chunk(c)\n",
    "        keys_to_delete = ['intake_esm_dataset_key', 'intake_esm_varname']\n",
    "        for k in keys_to_delete:\n",
    "            del ds.attrs[k]\n",
    "        dsets[key] = ds\n",
    "        variable = key.split('_')[-1]\n",
    "        print_ds_info(ds, variable)\n",
    "        if i == choice:\n",
    "            print(ds)\n",
    "        print('\\n')\n",
    "    return dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(ds):\n",
    "    \"\"\"Drop all unnecessary variables and coordinates\"\"\"\n",
    "    vars_to_drop = [vname for vname in ds.data_vars if vname not in variables]\n",
    "    coord_vars = [vname for vname in ds.data_vars if 'time' not in ds[vname].dims or 'bound' in vname]\n",
    "    ds_fixed = ds.set_coords(coord_vars)\n",
    "    data_vars_dims = []\n",
    "    for data_var in ds_fixed.data_vars:\n",
    "        data_vars_dims.extend(list(ds_fixed[data_var].dims))\n",
    "    coords_to_drop = [coord for coord in ds_fixed.coords if coord not in data_vars_dims]\n",
    "    grid_vars = list(set(vars_to_drop + coords_to_drop) - set(['time', 'time_bound']))\n",
    "    ds_fixed = ds_fixed.drop(grid_vars)\n",
    "    if 'history' in ds_fixed.attrs:\n",
    "        del ds_fixed.attrs['history']\n",
    "    return ds_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in run_config:\n",
    "    print(\"*\"*120)\n",
    "    print(f\"query = {run['query']}\")\n",
    "    frequency = run['frequency']\n",
    "    chunks = run['chunks']\n",
    "    dsets = run['col'].to_dataset_dict(cdf_kwargs={'chunks': chunks, 'decode_times': False}, preprocess=preprocess, progressbar=False)\n",
    "    dsets = enforce_chunking(dsets, chunks)\n",
    "    for key, ds in tqdm(dsets.items(), desc='Saving zarr store'):\n",
    "        key = key.split('_')\n",
    "        exp, cmp, var, frequency = key[1], key[0], key[-1], frequency\n",
    "        store = zarr_store(exp, cmp, frequency, var, write=True, dirout=dirout)\n",
    "        save_data(ds, store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the zarr stores were properly written\n",
    "\n",
    "from pathlib import Path\n",
    "p = Path(dirout) / 'ocn'\n",
    "stores = list(p.rglob(\"*.zarr\"))\n",
    "for store in stores:\n",
    "    try:\n",
    "        ds = xr.open_zarr(store.as_posix(), consolidated=True)\n",
    "        print('\\n')\n",
    "        print(store)\n",
    "        print(ds)\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        print(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext watermark\n",
    "# %watermark -d -iv -m -g -h"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lens-conversion]",
   "language": "python",
   "name": "conda-env-lens-conversion-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
