{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: https://github.com/NCAR/cesm-lens-aws/issues/34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pprint\n",
    "import random\n",
    "import shutil\n",
    "from functools import reduce\n",
    "from operator import mul\n",
    "\n",
    "import xarray as xr\n",
    "import yaml\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import dask\n",
    "import intake\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from distributed import Client\n",
    "from distributed.utils import format_bytes\n",
    "\n",
    "dask.config.set({\"distributed.dashboard.link\": \"/proxy/{port}/status\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = SLURMCluster(cores=4, memory=\"200GB\", project=\"STDD0003\")\n",
    "cluster.adapt(minimum_jobs=2, maximum_jobs=4)\n",
    "# cluster.scale(jobs=3)\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True if saving large Zarr files is resulting in KilledWorker or Dask crashes.\n",
    "BIG_SAVE = False\n",
    "if BIG_SAVE:\n",
    "    min_workers = 10\n",
    "    client.wait_for_workers(min_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ds_info(ds, var):\n",
    "    \"\"\"Function for printing chunking information\"\"\"\n",
    "    dt = ds[var].dtype\n",
    "    itemsize = dt.itemsize\n",
    "    chunk_size = ds[var].data.chunksize\n",
    "    size = format_bytes(ds.nbytes)\n",
    "    _bytes = reduce(mul, chunk_size) * itemsize\n",
    "    chunk_size_bytes = format_bytes(_bytes)\n",
    "\n",
    "    print(f\"Variable name: {var}\")\n",
    "    print(f\"Dataset dimensions: {ds[var].dims}\")\n",
    "    print(f\"Chunk shape: {chunk_size}\")\n",
    "    print(f\"Dataset shape: {ds[var].shape}\")\n",
    "    print(f\"Chunk size: {chunk_size_bytes}\")\n",
    "    print(f\"Dataset size: {size}\")\n",
    "\n",
    "\n",
    "dirout = \"/glade/work/bonnland/lens-aws\"\n",
    "\n",
    "\n",
    "def zarr_store(exp, cmp, frequency, var, write=False, dirout=dirout):\n",
    "    \"\"\" Create zarr store name/path\n",
    "    \"\"\"\n",
    "    path = f\"{dirout}/{cmp}/{frequency}/cesmLE-{exp}-{var}.zarr\"\n",
    "    if write and os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "    print(path)\n",
    "    return path\n",
    "\n",
    "\n",
    "def save_data(ds, store):\n",
    "    try:\n",
    "        ds.to_zarr(store, consolidated=True)\n",
    "        del ds\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to write {store}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's safer to use a dash '-' to separate fields, not underscores, because CESM variables have underscores.\n",
    "field_separator = \"-\"\n",
    "col = intake.open_esm_datastore(\n",
    "    \"../catalogs/glade-campaign-cesm1-le.json\", sep=field_separator,\n",
    ")\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_variables(col, variables, experiment, verbose=False):\n",
    "    query = dict(\n",
    "        component=component,\n",
    "        stream=stream,\n",
    "        variable=variables,\n",
    "        experiment=experiment,\n",
    "    )\n",
    "    subset = col.search(**query)\n",
    "    if verbose:\n",
    "        print(\n",
    "            subset.unique(\n",
    "                columns=[\"variable\", \"component\", \"stream\", \"experiment\"]\n",
    "            )\n",
    "        )\n",
    "    return subset, query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "run_config = []\n",
    "variables = []\n",
    "\n",
    "for component, stream_val in config.items():\n",
    "    for stream, v in stream_val.items():\n",
    "        frequency = v[\"frequency\"]\n",
    "        variable_categories = list(v[\"variable_category\"].keys())\n",
    "        for v_cat in variable_categories:\n",
    "            experiments = list(\n",
    "                v[\"variable_category\"][v_cat][\"experiment\"].keys()\n",
    "            )\n",
    "            for exp in experiments:\n",
    "                chunks = v[\"variable_category\"][v_cat][\"experiment\"][exp][\n",
    "                    \"chunks\"\n",
    "                ]\n",
    "                variable = v[\"variable_category\"][v_cat][\"variable\"]\n",
    "                variables.extend(variable)\n",
    "                col_subset, query = process_variables(col, variable, exp)\n",
    "                d = {\n",
    "                    \"query\": json.dumps(query),\n",
    "                    \"col\": col_subset,\n",
    "                    \"chunks\": chunks,\n",
    "                    \"frequency\": frequency,\n",
    "                }\n",
    "                run_config.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_chunking(datasets, chunks):\n",
    "    \"\"\"Enforce uniform chunking\"\"\"\n",
    "    dsets = datasets.copy()\n",
    "    choice = random.choice(range(0, len(dsets)))\n",
    "    for i, (key, ds) in enumerate(dsets.items()):\n",
    "        c = chunks.copy()\n",
    "        for dim in list(c):\n",
    "            if dim not in ds.dims:\n",
    "                del c[dim]\n",
    "        ds = ds.chunk(c)\n",
    "        keys_to_delete = [\"intake_esm_dataset_key\", \"intake_esm_varname\"]\n",
    "        for k in keys_to_delete:\n",
    "            del ds.attrs[k]\n",
    "        dsets[key] = ds\n",
    "        variable = key.split(field_separator)[-1]\n",
    "        print_ds_info(ds, variable)\n",
    "        if i == choice:\n",
    "            print(ds)\n",
    "        print(\"\\n\")\n",
    "    return dsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(ds):\n",
    "    \"\"\"Drop all unnecessary variables and coordinates\"\"\"\n",
    "    vars_to_drop = [vname for vname in ds.data_vars if vname not in variables]\n",
    "    coord_vars = [\n",
    "        vname\n",
    "        for vname in ds.data_vars\n",
    "        if \"time\" not in ds[vname].dims or \"bound\" in vname\n",
    "    ]\n",
    "    ds_fixed = ds.set_coords(coord_vars)\n",
    "    data_vars_dims = []\n",
    "    for data_var in ds_fixed.data_vars:\n",
    "        data_vars_dims.extend(list(ds_fixed[data_var].dims))\n",
    "    coords_to_drop = [\n",
    "        coord for coord in ds_fixed.coords if coord not in data_vars_dims\n",
    "    ]\n",
    "    grid_vars = list(\n",
    "        set(vars_to_drop + coords_to_drop) - set([\"time\", \"time_bound\"])\n",
    "    )\n",
    "    ds_fixed = ds_fixed.drop(grid_vars)\n",
    "    if \"history\" in ds_fixed.attrs:\n",
    "        del ds_fixed.attrs[\"history\"]\n",
    "    return ds_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in run_config:\n",
    "    print(\"*\" * 120)\n",
    "    print(f\"query = {run['query']}\")\n",
    "    frequency = run[\"frequency\"]\n",
    "    chunks = run[\"chunks\"]\n",
    "    dsets = run[\"col\"].to_dataset_dict(\n",
    "        cdf_kwargs={\"chunks\": chunks, \"decode_times\": False},\n",
    "        preprocess=preprocess,\n",
    "        progressbar=False,\n",
    "    )\n",
    "    dsets = enforce_chunking(dsets, chunks)\n",
    "    for key, ds in tqdm(dsets.items(), desc=\"Saving zarr store\"):\n",
    "        key = key.split(field_separator)\n",
    "        exp, cmp, var, frequency = key[1], key[0], key[-1], frequency\n",
    "        store = zarr_store(exp, cmp, frequency, var, write=True, dirout=dirout)\n",
    "        save_data(ds, store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the zarr stores were properly written\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "p = Path(dirout) / \"ocn\"\n",
    "stores = list(p.rglob(\"*.zarr\"))\n",
    "for store in stores:\n",
    "    try:\n",
    "        ds = xr.open_zarr(store.as_posix(), consolidated=True)\n",
    "        print(\"\\n\")\n",
    "        print(store)\n",
    "        print(ds)\n",
    "    except Exception as e:\n",
    "        # print(e)\n",
    "        print(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext watermark\n",
    "# %watermark -d -iv -m -g -h"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
