{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zarrify DART Reanalysis History Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import intake\n",
    "import ast\n",
    "\n",
    "import dask.distributed\n",
    "from dask.distributed import Client\n",
    "from ncar_jobqueue import NCARCluster\n",
    "\n",
    "import fsspec\n",
    "from pathlib import Path\n",
    "import shutil \n",
    "import os\n",
    "from functools import reduce\n",
    "from operator import mul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_path = './dart-zarr-input.json'\n",
    "\n",
    "output_folder = '/glade/scratch/bonnland/DART/ds345.0/zarr-publish'\n",
    "\n",
    "#output_variables = ['T', 'PS', 'Q', 'US', 'VS', 'CLDICE', 'CLDLIQ']\n",
    "\n",
    "output_variables = ['CLDICE', 'CLDLIQ', 'VS']\n",
    "\n",
    "# Number of elements per chunk in the target stores.\n",
    "# A negative value means \"don't chunk this dimension\".\n",
    "target_chunks = {'lat': 32, \n",
    "                 'slat': 32, \n",
    "                 'lon': 32, \n",
    "                 'slon': 32, \n",
    "                 'lev': -1,\n",
    "                 'time': 30, \n",
    "                 'member_id': 10}\n",
    "\n",
    "# target_chunks = {'lat': 32, \n",
    "#                  'lon': 32, \n",
    "#                  'lev': -1,\n",
    "#                  'time': 30, \n",
    "#                  'member_id': 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run These Cells for Dask Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from ncar_jobqueue import NCARCluster\n",
    "\n",
    "# For Cheyenne\n",
    "\n",
    "# These are \"per node\", and then .scale() selects the number of nodes.\n",
    "#walltime = \"1:00:00\"\n",
    "#walltime = \"00:30:00\"\n",
    "walltime = \"00:45:00\"\n",
    "\n",
    "#  This results in about 20% maximum memory usage.\n",
    "#cluster = NCARCluster(cores=1, processes=1, memory='109GB', walltime=walltime)\n",
    "#num_nodes = 16\n",
    "\n",
    "\n",
    "# Run 16 workers on 4 nodes, giving each worker around 25GB RAM.  \n",
    "#cluster = NCARCluster(cores=4, processes=4, memory='109GB', walltime=walltime)\n",
    "\n",
    "# # Run 4 workers on each node, giving each worker around 25GB RAM.  \n",
    "# cluster = NCARCluster(cores=16, processes=4, memory='109GB', walltime=walltime)\n",
    "# num_nodes = 2\n",
    "\n",
    "# Run <= 4 workers on each node to avoid crashes.\n",
    "cluster = NCARCluster(cores=10, processes=4, memory='109GB', walltime=walltime)\n",
    "num_nodes = 8\n",
    "\n",
    "cluster.scale(jobs=num_nodes)\n",
    "\n",
    "from distributed import Client\n",
    "from distributed.utils import format_bytes\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zarr-related Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing Steps for each input dataset before merge\n",
    "def preprocess(ds):\n",
    "    \"\"\"Pare down each input dataset to a single variable.  \n",
    "       The subsequent merge will eliminate unused coordinates automatically. \n",
    "        \n",
    "        This function does not allow additional arguments, so the target \n",
    "        output variable needs to be defined globally in TARGET_VAR.\n",
    "    \"\"\"\n",
    "    drop_vars = [var for var in ds.data_vars \n",
    "                 if var != TARGET_VAR]\n",
    "\n",
    "    ds_fixed = ds.drop_vars(drop_vars)\n",
    "    \n",
    "    return ds_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_ds_info(ds, var):\n",
    "    \"\"\"Function for printing chunking information\"\"\"\n",
    "\n",
    "    print(f'print_ds_info: var == {var}')\n",
    "    dt = ds[var].dtype\n",
    "    itemsize = dt.itemsize\n",
    "    chunk_size = ds[var].data.chunksize\n",
    "    size = format_bytes(ds.nbytes)\n",
    "    _bytes = reduce(mul, chunk_size) * itemsize\n",
    "    chunk_size_bytes = format_bytes(_bytes)\n",
    "\n",
    "    print(f'Variable name: {var}')\n",
    "    print(f'Dataset dimensions: {ds[var].dims}')\n",
    "    print(f'Chunk shape: {chunk_size}')\n",
    "    print(f'Dataset shape: {ds[var].shape}')\n",
    "    print(f'Chunk size: {chunk_size_bytes}')\n",
    "    print(f'Dataset size: {size}')\n",
    "\n",
    "    \n",
    "def zarr_store(var, dirout, write=False):\n",
    "    \"\"\" Create zarr store name/path\n",
    "    \"\"\"\n",
    "    path = f'{dirout}/{var}.zarr'\n",
    "    if write and os.path.exists(path):\n",
    "        shutil.rmtree(path)\n",
    "    print(path)\n",
    "    return path\n",
    "\n",
    "\n",
    "def save_data(ds, store):\n",
    "    try:\n",
    "        ds.to_zarr(store=store, consolidated=True)\n",
    "        del ds\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to write {store}: {e}\")\n",
    "\n",
    "        \n",
    "def zarr_check():\n",
    "    '''Make sure the zarr stores were properly written'''\n",
    "\n",
    "    from pathlib import Path\n",
    "    p = Path(output_folder)\n",
    "    stores = list(p.rglob(\"*.zarr\"))\n",
    "    #stores = list(p.rglob(\"*.rcp45.day.NAM-22i.raw.zarr\"))\n",
    "    for store in stores:\n",
    "        try:\n",
    "            ds = xr.open_zarr(store.as_posix(), consolidated=True)\n",
    "            print('\\n')\n",
    "            print(store)\n",
    "            print(ds)\n",
    "        except Exception as e:\n",
    "            #print(e)\n",
    "            print(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open catalog with single-valued \"variable\" column\n",
    "\n",
    "# Have the catalog interpret the \"variable\" column as a list of values.\n",
    "col = intake.open_esm_datastore(catalog_path)\n",
    "col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the eventual output store base names.\n",
    "print(\"Eventual store base names:\")\n",
    "print(col.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REALLY_SAVE = True\n",
    "\n",
    "for variable in output_variables:\n",
    "    # This variable gets used in the \"preprocess\" function and must be defined now in the global scope.\n",
    "    TARGET_VAR = variable\n",
    "\n",
    "    col_subset = col.search(variable = variable)\n",
    "    # Produce var-based stores.  The catalog will determine how many stores and their base names.\n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "        dsets = col_subset.to_dataset_dict(zarr_kwargs={'consolidated': True}, preprocess=preprocess)\n",
    "\n",
    "    ds_out = dsets[variable]\n",
    "    \n",
    "    # Specify final chunking.\n",
    "    ds_out = ds_out.chunk(target_chunks)\n",
    "    \n",
    "    # Confirm output contents.\n",
    "    print_ds_info(ds_out, variable)\n",
    "    \n",
    "    store = zarr_store(variable, dirout = output_folder, write=REALLY_SAVE)\n",
    "    if REALLY_SAVE:\n",
    "        save_data(ds_out, store=store)\n",
    "        print(\"     ... Done.\")\n",
    "    else:\n",
    "        print(\"     ... (Skipping)\")\n",
    "        del ds_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open each output dataset to confirm it was created properly.\n",
    "\n",
    "zarr_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
